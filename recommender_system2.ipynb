{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contain the multiple attempts and method tried for recommending \n",
    "a company based on a query "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first decide to use TF IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sevencorners'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Re-importing the dataset due to session reset\n",
    "file_path = 'cleaned_reviews2.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Replace missing or non-string values with an empty string\n",
    "df['Cleaned_Review'] = df['Cleaned_Review'].apply(lambda x: '' if not isinstance(x, str) else x)\n",
    "\n",
    "# Aggregating reviews by company\n",
    "aggregated_reviews = df.groupby('Name')['Cleaned_Review'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "\n",
    "# Initialize TF-IDF Vectorizer to extract keywords\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.85, stop_words='english', use_idf=True, norm='l2')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(aggregated_reviews['Cleaned_Review'])\n",
    "\n",
    "# Extract feature names (words)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Function to extract top N keywords for each document (company)\n",
    "def extract_top_keywords(tfidf_matrix, feature_names, top_n=20):\n",
    "    keywords = []\n",
    "    for row in tfidf_matrix:\n",
    "        words = [feature_names[idx] for idx in row.indices]\n",
    "        scores = row.data\n",
    "        sorted_keywords = sorted(zip(words, scores), key=lambda x: -x[1])[:top_n]\n",
    "        keywords.append([word for word, score in sorted_keywords])\n",
    "    return keywords\n",
    "\n",
    "# Extract top keywords for each company\n",
    "top_keywords = extract_top_keywords(tfidf_matrix, feature_names)\n",
    "aggregated_reviews['Keywords'] = top_keywords\n",
    "\n",
    "# Create a new TF-IDF Vectorizer using the keyword summaries\n",
    "tfidf_vectorizer_keywords = TfidfVectorizer()\n",
    "tfidf_matrix_keywords = tfidf_vectorizer_keywords.fit_transform(aggregated_reviews['Keywords'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "# User query\n",
    "# user_query = \"looking for insurance\"\n",
    "user_query = \"looking for insurance for travelling\"\n",
    "\n",
    "# Transform the user query using the same vectorizer\n",
    "query_vector_keywords = tfidf_vectorizer_keywords.transform([user_query])\n",
    "\n",
    "# Calculate cosine similarity between the user query and the keyword vectors\n",
    "cosine_similarities_keywords = cosine_similarity(query_vector_keywords, tfidf_matrix_keywords).flatten()\n",
    "\n",
    "# Find the index of the highest similarity score\n",
    "highest_score_index_keywords = cosine_similarities_keywords.argmax()\n",
    "\n",
    "# Recommend the company with the highest cosine similarity score\n",
    "recommended_company_keywords = aggregated_reviews.iloc[highest_score_index_keywords]['Name']\n",
    "recommended_company_keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AARDY'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Re-importing the dataset due to session reset\n",
    "file_path = 'cleaned_reviews2.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Replace missing or non-string values with an empty string\n",
    "df['Cleaned_Review'] = df['Cleaned_Review'].apply(lambda x: '' if not isinstance(x, str) else x)\n",
    "\n",
    "# Aggregating reviews by company\n",
    "aggregated_reviews = df.groupby('Name')['Cleaned_Review'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "\n",
    "# Initialize TF-IDF Vectorizer to extract keywords\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.85, stop_words='english', use_idf=True, norm='l2')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(aggregated_reviews['Cleaned_Review'])\n",
    "\n",
    "# Extract feature names (words)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Function to extract top N keywords for each document (company)\n",
    "def extract_top_keywords(tfidf_matrix, feature_names, top_n=10):\n",
    "    keywords = []\n",
    "    for row in tfidf_matrix:\n",
    "        words = [feature_names[idx] for idx in row.indices]\n",
    "        scores = row.data\n",
    "        sorted_keywords = sorted(zip(words, scores), key=lambda x: -x[1])[:top_n]\n",
    "        keywords.append([word for word, score in sorted_keywords])\n",
    "    return keywords\n",
    "\n",
    "# Extract top keywords for each company\n",
    "top_keywords = extract_top_keywords(tfidf_matrix, feature_names)\n",
    "aggregated_reviews['Keywords'] = top_keywords\n",
    "\n",
    "# Create a new TF-IDF Vectorizer using the keyword summaries\n",
    "tfidf_vectorizer_keywords = TfidfVectorizer()\n",
    "tfidf_matrix_keywords = tfidf_vectorizer_keywords.fit_transform(aggregated_reviews['Keywords'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "# User query\n",
    "# user_query = \"looking for insurance\"\n",
    "user_query = \"looking for insurance AARDY\"\n",
    "\n",
    "# Transform the user query using the same vectorizer\n",
    "query_vector_keywords = tfidf_vectorizer_keywords.transform([user_query])\n",
    "\n",
    "# Calculate cosine similarity between the user query and the keyword vectors\n",
    "cosine_similarities_keywords = cosine_similarity(query_vector_keywords, tfidf_matrix_keywords).flatten()\n",
    "\n",
    "# Find the index of the highest similarity score\n",
    "highest_score_index_keywords = cosine_similarities_keywords.argmax()\n",
    "\n",
    "# Recommend the company with the highest cosine similarity score\n",
    "recommended_company_keywords = aggregated_reviews.iloc[highest_score_index_keywords]['Name']\n",
    "recommended_company_keywords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we try our data on another subset of our dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'easyautoonline'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Re-importing the dataset due to session reset\n",
    "file_path = 'recommandation.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Replace missing or non-string values with an empty string\n",
    "df['Review'] = df['Review'].apply(lambda x: '' if not isinstance(x, str) else x)\n",
    "\n",
    "# Aggregating reviews by company\n",
    "aggregated_reviews = df.groupby('Name')['Review'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "\n",
    "# Initialize TF-IDF Vectorizer to extract keywords\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.85, stop_words='english', use_idf=True, norm='l2')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(aggregated_reviews['Review'])\n",
    "\n",
    "# Extract feature names (words)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "\n",
    "# Function to extract top N keywords for each document (company)\n",
    "def extract_top_keywords(tfidf_matrix, feature_names, top_n=10):\n",
    "    keywords = []\n",
    "    for row in tfidf_matrix:\n",
    "        words = [feature_names[idx] for idx in row.indices]\n",
    "        scores = row.data\n",
    "        sorted_keywords = sorted(zip(words, scores), key=lambda x: -x[1])[:top_n]\n",
    "        keywords.append([word for word, score in sorted_keywords])\n",
    "    return keywords\n",
    "\n",
    "# Extract top keywords for each company\n",
    "top_keywords = extract_top_keywords(tfidf_matrix, feature_names)\n",
    "aggregated_reviews['Keywords'] = top_keywords\n",
    "\n",
    "# Create a new TF-IDF Vectorizer using the keyword summaries\n",
    "tfidf_vectorizer_keywords = TfidfVectorizer()\n",
    "tfidf_matrix_keywords = tfidf_vectorizer_keywords.fit_transform(aggregated_reviews['Keywords'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "# User query\n",
    "# user_query = \"looking for insurance\"\n",
    "# user_query = \"looking for a car insurance\"\n",
    "# user_query = \"looking for a car insurance\"\n",
    "user_query = \"car for travelling\"\n",
    "\n",
    "# Transform the user query using the same vectorizer\n",
    "query_vector_keywords = tfidf_vectorizer_keywords.transform([user_query])\n",
    "\n",
    "# Calculate cosine similarity between the user query and the keyword vectors\n",
    "cosine_similarities_keywords = cosine_similarity(query_vector_keywords, tfidf_matrix_keywords).flatten()\n",
    "\n",
    "# Find the index of the highest similarity score\n",
    "highest_score_index_keywords = cosine_similarities_keywords.argmax()\n",
    "\n",
    "# Recommend the company with the highest cosine similarity score\n",
    "recommended_company_keywords = aggregated_reviews.iloc[highest_score_index_keywords]['Name']\n",
    "recommended_company_keywords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we tried our "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['afford' 'affordability' 'affordable' 'affordably' 'afordable' 'afraid'\n",
      " 'africa' 'african' 'aft' 'afte']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Roamright'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Re-importing the dataset due to session reset\n",
    "file_path = 'recommandation.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Replace missing or non-string values with an empty string\n",
    "df['Review'] = df['Review'].apply(lambda x: '' if not isinstance(x, str) else x)\n",
    "\n",
    "# Aggregating reviews by company\n",
    "aggregated_reviews = df.groupby('Name')['Review'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "\n",
    "# Initialize TF-IDF Vectorizer to extract keywords\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.85, stop_words='english', use_idf=True, norm='l2')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(aggregated_reviews['Review'])\n",
    "\n",
    "# Extract feature names (words)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "\n",
    "# Function to extract top N keywords for each document (company)\n",
    "def extract_top_keywords(tfidf_matrix, feature_names, top_n=10):\n",
    "    keywords = []\n",
    "    for row in tfidf_matrix:\n",
    "        words = [feature_names[idx] for idx in row.indices]\n",
    "        scores = row.data\n",
    "        sorted_keywords = sorted(zip(words, scores), key=lambda x: -x[1])[:top_n]\n",
    "        keywords.append([word for word, score in sorted_keywords])\n",
    "    return keywords\n",
    "\n",
    "# Extract top keywords for each company\n",
    "top_keywords = extract_top_keywords(tfidf_matrix, feature_names)\n",
    "aggregated_reviews['Keywords'] = top_keywords\n",
    "\n",
    "# Create a new TF-IDF Vectorizer using the keyword summaries\n",
    "tfidf_vectorizer_keywords = TfidfVectorizer()\n",
    "tfidf_matrix_keywords = tfidf_vectorizer_keywords.fit_transform(aggregated_reviews['Keywords'].apply(lambda x: ' '.join(x)))\n",
    "print(feature_names[500:510])\n",
    "# User query\n",
    "# user_query = \"looking for insurance\"\n",
    "# user_query = \"looking for a car insurance\"\n",
    "# user_query = \"looking for a car insurance\"\n",
    "user_query = \"\"\n",
    "\n",
    "# Transform the user query using the same vectorizer\n",
    "query_vector_keywords = tfidf_vectorizer_keywords.transform([user_query])\n",
    "\n",
    "# Calculate cosine similarity between the user query and the keyword vectors\n",
    "cosine_similarities_keywords = cosine_similarity(query_vector_keywords, tfidf_matrix_keywords).flatten()\n",
    "\n",
    "# Find the index of the highest similarity score\n",
    "highest_score_index_keywords = cosine_similarities_keywords.argmax()\n",
    "\n",
    "# Recommend the company with the highest cosine similarity score\n",
    "recommended_company_keywords = aggregated_reviews.iloc[highest_score_index_keywords]['Name']\n",
    "recommended_company_keywords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we try BM25 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'drivetime'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from math import log\n",
    "\n",
    "\n",
    "\n",
    "# Function to compute BM25 for a given document and query\n",
    "def bm25(doc, query, k1=1.5, b=0.75, epsilon=0.25):\n",
    "    \"\"\"\n",
    "    Compute BM25 score for a single document and a query.\n",
    "    :param doc: list of words in the document\n",
    "    :param query: list of words in the query\n",
    "    :param k1, b, epsilon: BM25 parameters\n",
    "    :return: BM25 score\n",
    "    \"\"\"\n",
    "    # Calculate average document length\n",
    "    avgdl = sum(len(d) for d in doc) / len(doc)\n",
    "    \n",
    "    # Compute IDF for each query term\n",
    "    N = len(doc)\n",
    "    idf = {}\n",
    "    for term in np.unique(query):\n",
    "        n_qi = sum(term in d for d in doc)\n",
    "        idf[term] = log((N - n_qi + 0.5) / (n_qi + 0.5) + 1)\n",
    "    \n",
    "    # Compute BM25 score\n",
    "    doc_counter = Counter(doc)\n",
    "    score = 0\n",
    "    for term in query:\n",
    "        if term in doc_counter:\n",
    "            df = doc_counter[term]\n",
    "            score += idf[term] * df * (k1 + 1) / (df + k1 * (1 - b + b * len(doc) / avgdl))\n",
    "\n",
    "    return score\n",
    "\n",
    "# Convert keyword summaries to lists of words for BM25 calculation\n",
    "document_keywords = [doc.split() for doc in aggregated_reviews['Keywords'].apply(lambda x: ' '.join(x))]\n",
    "\n",
    "user_query = \"i want a car\"\n",
    "\n",
    "\n",
    "# User query as a list of words\n",
    "user_query_list = user_query.split()\n",
    "\n",
    "# Calculate BM25 score for each document (company) against the user query\n",
    "bm25_scores = [bm25(doc, user_query_list) for doc in document_keywords]\n",
    "\n",
    "# Find the index of the highest BM25 score\n",
    "highest_bm25_score_index = np.argmax(bm25_scores)\n",
    "\n",
    "# Recommend the company with the highest BM25 score\n",
    "recommended_company_bm25 = aggregated_reviews.iloc[highest_bm25_score_index]['Name']\n",
    "recommended_company_bm25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hugod\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "C:\\Users\\hugod\\AppData\\Local\\Temp\\ipykernel_6468\\3551234045.py:39: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
      "  similarities = model.docvecs.most_similar([user_query_vector], topn=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'oiseaux-mania'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Re-importing the dataset due to session reset\n",
    "file_path = 'recommandation3.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Replace missing or non-string values with an empty string\n",
    "df['Review'] = df['Review'].apply(lambda x: '' if not isinstance(x, str) else x)\n",
    "\n",
    "# Aggregating reviews by company\n",
    "aggregated_reviews = df.groupby('Name')['Review'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Prepare the data for Doc2Vec (each document should be a list of words)\n",
    "tagged_data = [TaggedDocument(words=word_tokenize(doc.lower()), tags=[str(i)]) for i, doc in enumerate(aggregated_reviews['Review'])]\n",
    "\n",
    "# Train a Doc2Vec model\n",
    "model = Doc2Vec(vector_size=100, window=2, min_count=1, workers=4, epochs=40)\n",
    "model.build_vocab(tagged_data)\n",
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "# User query\n",
    "user_query = \"i want insurance\"\n",
    "\n",
    "# Tokenize and infer the vector for the user query\n",
    "user_query_vector = model.infer_vector(word_tokenize(user_query.lower()))\n",
    "\n",
    "# Calculate similarities\n",
    "similarities = model.docvecs.most_similar([user_query_vector], topn=1)\n",
    "\n",
    "# Get the index of the most similar document\n",
    "most_similar_doc_index = int(similarities[0][0])\n",
    "\n",
    "# Recommend the company\n",
    "recommended_company_doc2vec = aggregated_reviews.iloc[most_similar_doc_index]['Name']\n",
    "recommended_company_doc2vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'selininy'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Re-importing the dataset due to session reset\n",
    "file_path = 'recommandation.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Replace missing or non-string values with an empty string\n",
    "df['Review'] = df['Review'].apply(lambda x: '' if not isinstance(x, str) else x)\n",
    "\n",
    "# Aggregating reviews by company\n",
    "aggregated_reviews = df.groupby('Name')['Review'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "\n",
    "# Load pre-trained model tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to encode text using BERT\n",
    "def encode_text(text):\n",
    "    # Tokenize and encode the text\n",
    "    input_ids = tokenizer.encode(text, add_special_tokens=True, max_length=512, truncation=True)\n",
    "    input_ids = torch.tensor([input_ids])\n",
    "    \n",
    "    # Get the embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        last_hidden_states = outputs.last_hidden_state\n",
    "        # We take the mean of the last hidden states as the sentence representation\n",
    "        sentence_embedding = torch.mean(last_hidden_states, dim=1)\n",
    "    return sentence_embedding\n",
    "\n",
    "# Encode the user query\n",
    "# user_query = \"looking for insurance\"\n",
    "user_query = \"looking for a nice dress \"\n",
    "user_query_embedding = encode_text(user_query)\n",
    "\n",
    "# Encode the aggregated reviews for each company\n",
    "company_embeddings = aggregated_reviews['Review'].apply(lambda x: encode_text(x).numpy())\n",
    "\n",
    "# Calculate cosine similarity between the user query and each company's reviews\n",
    "cosine_similarities = [cosine_similarity(user_query_embedding, company_embedding)[0][0] for company_embedding in company_embeddings]\n",
    "\n",
    "# Find the index of the highest similarity score\n",
    "highest_score_index = cosine_similarities.index(max(cosine_similarities))\n",
    "\n",
    "# Recommend the company with the highest cosine similarity score\n",
    "recommended_company_bert = aggregated_reviews.iloc[highest_score_index]['Name']\n",
    "recommended_company_bert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sevencorners'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_query = \"looking for a car insurance\"\n",
    "user_query_embedding = encode_text(user_query)\n",
    "\n",
    "# Encode the aggregated reviews for each company\n",
    "company_embeddings = aggregated_reviews['Review'].apply(lambda x: encode_text(x).numpy())\n",
    "\n",
    "# Calculate cosine similarity between the user query and each company's reviews\n",
    "cosine_similarities = [cosine_similarity(user_query_embedding, company_embedding)[0][0] for company_embedding in company_embeddings]\n",
    "\n",
    "# Find the index of the highest similarity score\n",
    "highest_score_index = cosine_similarities.index(max(cosine_similarities))\n",
    "\n",
    "# Recommend the company with the highest cosine similarity score\n",
    "recommended_company_bert = aggregated_reviews.iloc[highest_score_index]['Name']\n",
    "recommended_company_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'petrebellion'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Re-importing the dataset due to session reset\n",
    "file_path = 'recommandation4.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Replace missing or non-string values with an empty string\n",
    "df['Review'] = df['Review'].apply(lambda x: '' if not isinstance(x, str) else x)\n",
    "\n",
    "# Aggregating reviews by company\n",
    "aggregated_reviews = df.groupby('Name')['Review'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "\n",
    "# Load pre-trained model tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to encode text using BERT\n",
    "def encode_text(text):\n",
    "    # Tokenize and encode the text\n",
    "    input_ids = tokenizer.encode(text, add_special_tokens=True, max_length=512, truncation=True)\n",
    "    input_ids = torch.tensor([input_ids])\n",
    "    \n",
    "    # Get the embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        last_hidden_states = outputs.last_hidden_state\n",
    "        # We take the mean of the last hidden states as the sentence representation\n",
    "        sentence_embedding = torch.mean(last_hidden_states, dim=1)\n",
    "    return sentence_embedding\n",
    "\n",
    "# Encode the user query\n",
    "# user_query = \"looking for insurance\"\n",
    "user_query = \"looking for my pet \"\n",
    "user_query_embedding = encode_text(user_query)\n",
    "\n",
    "# Encode the aggregated reviews for each company\n",
    "company_embeddings = aggregated_reviews['Review'].apply(lambda x: encode_text(x).numpy())\n",
    "\n",
    "# Calculate cosine similarity between the user query and each company's reviews\n",
    "cosine_similarities = [cosine_similarity(user_query_embedding, company_embedding)[0][0] for company_embedding in company_embeddings]\n",
    "\n",
    "# Find the index of the highest similarity score\n",
    "highest_score_index = cosine_similarities.index(max(cosine_similarities))\n",
    "\n",
    "# Recommend the company with the highest cosine similarity score\n",
    "recommended_company_bert = aggregated_reviews.iloc[highest_score_index]['Name']\n",
    "recommended_company_bert\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we try with Doc2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hugod\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "C:\\Users\\hugod\\AppData\\Local\\Temp\\ipykernel_9108\\1618794734.py:38: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
      "  similarities = model.docvecs.most_similar([user_query_vector], topn=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'selininy'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Re-importing the dataset due to session reset\n",
    "file_path = 'recommandation4.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Replace missing or non-string values with an empty string\n",
    "df['Review'] = df['Review'].apply(lambda x: '' if not isinstance(x, str) else x)\n",
    "\n",
    "# Aggregating reviews by company\n",
    "aggregated_reviews = df.groupby('Name')['Review'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Prepare the data for Doc2Vec (each document should be a list of words)\n",
    "tagged_data = [TaggedDocument(words=word_tokenize(doc.lower()), tags=[str(i)]) for i, doc in enumerate(aggregated_reviews['Review'])]\n",
    "\n",
    "# Train a Doc2Vec model\n",
    "model = Doc2Vec(vector_size=100, window=2, min_count=1, workers=4, epochs=40)\n",
    "model.build_vocab(tagged_data)\n",
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "# User query\n",
    "user_query = \"i want a bird\"\n",
    "\n",
    "# Tokenize and infer the vector for the user query\n",
    "user_query_vector = model.infer_vector(word_tokenize(user_query.lower()))\n",
    "\n",
    "# Calculate similarities\n",
    "similarities = model.docvecs.most_similar([user_query_vector], topn=1)\n",
    "\n",
    "# Get the index of the most similar document\n",
    "most_similar_doc_index = int(similarities[0][0])\n",
    "\n",
    "# Recommend the company\n",
    "recommended_company_doc2vec = aggregated_reviews.iloc[most_similar_doc_index]['Name']\n",
    "recommended_company_doc2vec\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
