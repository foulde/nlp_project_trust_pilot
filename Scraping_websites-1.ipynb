{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOXjGKfK9E2h"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "import  bs4"
      ],
      "metadata": {
        "id": "8JoXMrla9b37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Travel insurance"
      ],
      "metadata": {
        "id": "Pd7ajaSLaglq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Website AARDY"
      ],
      "metadata": {
        "id": "7g5pZhA_9fqz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "name = \"AARDY\"\n",
        "base_url = 'https://www.trustpilot.com/review/aardy.com?page={}'\n",
        "\n",
        "def scrape_reviews(url, csv_writer):\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        section_elements = soup.find_all('section', class_='styles_reviewContentwrapper__zH_9M')\n",
        "\n",
        "        for i, section_element in enumerate(section_elements, start=1):\n",
        "            review = section_element.find('p', class_='typography_body-l__KUYFJ typography_appearance-default__AAY17 typography_color-black__5LYEn')\n",
        "            stars = section_element.find('div', class_='styles_reviewHeader__iU9Px')\n",
        "\n",
        "            if review:\n",
        "                review_text = review.get_text(strip=True)\n",
        "            else:\n",
        "                review_text = \"No review text found\"\n",
        "\n",
        "            if stars:\n",
        "                number_of_stars = stars.get('data-service-review-rating')\n",
        "            else:\n",
        "                number_of_stars = \"No star rating found\"\n",
        "\n",
        "            csv_writer.writerow([name, review_text, number_of_stars])\n",
        "\n",
        "csv_filename = 'AARDY.csv'\n",
        "with open(csv_filename, 'w', encoding='utf-8', newline='') as csvfile:\n",
        "\n",
        "    csv_writer = csv.writer(csvfile)\n",
        "    csv_writer.writerow(['Name', 'Review', 'Number of Stars'])\n",
        "\n",
        "    for page_number in range(1, 101):\n",
        "        url = base_url.format(page_number)\n",
        "        scrape_reviews(url, csv_writer)\n",
        "\n",
        "print(f'Data has been written to {csv_filename}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjnYr6Wa9b1y",
        "outputId": "5322f606-6e02-44a6-e529-13e75f1f16a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data has been written to AARDY.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Website Sevencorners"
      ],
      "metadata": {
        "id": "dMET4ZP8-UG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "name = \"Sevencorners\"\n",
        "base_url = 'https://www.trustpilot.com/review/sevencorners.com?page={}'\n",
        "\n",
        "def scrape_reviews(url, csv_writer):\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        section_elements = soup.find_all('section', class_='styles_reviewContentwrapper__zH_9M')\n",
        "\n",
        "        for i, section_element in enumerate(section_elements, start=1):\n",
        "            review = section_element.find('p', class_='typography_body-l__KUYFJ typography_appearance-default__AAY17 typography_color-black__5LYEn')\n",
        "            stars = section_element.find('div', class_='styles_reviewHeader__iU9Px')\n",
        "\n",
        "            if review:\n",
        "                review_text = review.get_text(strip=True)\n",
        "            else:\n",
        "                review_text = \"No review text found\"\n",
        "\n",
        "            if stars:\n",
        "                number_of_stars = stars.get('data-service-review-rating')\n",
        "            else:\n",
        "                number_of_stars = \"No star rating found\"\n",
        "\n",
        "            csv_writer.writerow([name, review_text, number_of_stars])\n",
        "\n",
        "csv_filename = 'Sevencorners.csv'\n",
        "with open(csv_filename, 'w', encoding='utf-8', newline='') as csvfile:\n",
        "\n",
        "    csv_writer = csv.writer(csvfile)\n",
        "    csv_writer.writerow(['Name', 'Review', 'Number of Stars'])\n",
        "\n",
        "    for page_number in range(1, 101):\n",
        "        url = base_url.format(page_number)\n",
        "        scrape_reviews(url, csv_writer)\n",
        "\n",
        "print(f'Data has been written to {csv_filename}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsTWmaAO9bxu",
        "outputId": "0ab0f3eb-55f3-4a71-a5e7-99a67c3a6bb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data has been written to Sevencorners.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Website Roamright"
      ],
      "metadata": {
        "id": "q4PLh9aH_0hv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "name = \"Roamright\"\n",
        "base_url = 'https://www.trustpilot.com/review/www.roamright.com?page={}'\n",
        "\n",
        "def scrape_reviews(url, csv_writer):\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        section_elements = soup.find_all('section', class_='styles_reviewContentwrapper__zH_9M')\n",
        "\n",
        "        for i, section_element in enumerate(section_elements, start=1):\n",
        "            review = section_element.find('p', class_='typography_body-l__KUYFJ typography_appearance-default__AAY17 typography_color-black__5LYEn')\n",
        "            stars = section_element.find('div', class_='styles_reviewHeader__iU9Px')\n",
        "\n",
        "            if review:\n",
        "                review_text = review.get_text(strip=True)\n",
        "            else:\n",
        "                review_text = \"No review text found\"\n",
        "\n",
        "            if stars:\n",
        "                number_of_stars = stars.get('data-service-review-rating')\n",
        "            else:\n",
        "                number_of_stars = \"No star rating found\"\n",
        "\n",
        "            csv_writer.writerow([name, review_text, number_of_stars])\n",
        "\n",
        "csv_filename = 'Roamright.csv'\n",
        "with open(csv_filename, 'w', encoding='utf-8', newline='') as csvfile:\n",
        "\n",
        "    csv_writer = csv.writer(csvfile)\n",
        "    csv_writer.writerow(['Name', 'Review', 'Number of Stars'])\n",
        "\n",
        "    for page_number in range(1, 76):\n",
        "        url = base_url.format(page_number)\n",
        "        scrape_reviews(url, csv_writer)\n",
        "\n",
        "print(f'Data has been written to {csv_filename}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IRf9U5-_Ywl",
        "outputId": "2a2a2419-d998-4e54-b52e-7da668b06645"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data has been written to Roamright.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Website webuyanycarusa"
      ],
      "metadata": {
        "id": "J0fnb-q5Jfws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "name = \"webuyanycarusa\"\n",
        "base_url = 'https://www.trustpilot.com/review/webuyanycarusa.com?page={}'\n",
        "\n",
        "def scrape_reviews(url, csv_writer):\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        section_elements = soup.find_all('section', class_='styles_reviewContentwrapper__zH_9M')\n",
        "\n",
        "        for i, section_element in enumerate(section_elements, start=1):\n",
        "            review = section_element.find('p', class_='typography_body-l__KUYFJ typography_appearance-default__AAY17 typography_color-black__5LYEn')\n",
        "            stars = section_element.find('div', class_='styles_reviewHeader__iU9Px')\n",
        "\n",
        "            if review:\n",
        "                review_text = review.get_text(strip=True)\n",
        "            else:\n",
        "                review_text = \"No review text found\"\n",
        "\n",
        "            if stars:\n",
        "                number_of_stars = stars.get('data-service-review-rating')\n",
        "            else:\n",
        "                number_of_stars = \"No star rating found\"\n",
        "\n",
        "            csv_writer.writerow([name, review_text, number_of_stars])\n",
        "\n",
        "csv_filename = 'webuyanycarusa.csv'\n",
        "with open(csv_filename, 'w', encoding='utf-8', newline='') as csvfile:\n",
        "\n",
        "    csv_writer = csv.writer(csvfile)\n",
        "    csv_writer.writerow(['Name', 'Review', 'Number of Stars'])\n",
        "\n",
        "    for page_number in range(1, 51):\n",
        "        url = base_url.format(page_number)\n",
        "        scrape_reviews(url, csv_writer)\n",
        "\n",
        "print(f'Data has been written to {csv_filename}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOVhnqoWI41C",
        "outputId": "bc58269b-e1be-462a-b00b-58874b0d335f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data has been written to webuyanycarusa.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Car dealer"
      ],
      "metadata": {
        "id": "Ys7ecfataciq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Website drivetime"
      ],
      "metadata": {
        "id": "GCFvY59kNlxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "name = \"drivetime\"\n",
        "base_url = 'https://www.trustpilot.com/review/www.drivetime.com?page={}'\n",
        "\n",
        "def scrape_reviews(url, csv_writer):\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        section_elements = soup.find_all('section', class_='styles_reviewContentwrapper__zH_9M')\n",
        "\n",
        "        for i, section_element in enumerate(section_elements, start=1):\n",
        "            review = section_element.find('p', class_='typography_body-l__KUYFJ typography_appearance-default__AAY17 typography_color-black__5LYEn')\n",
        "            stars = section_element.find('div', class_='styles_reviewHeader__iU9Px')\n",
        "\n",
        "            if review:\n",
        "                review_text = review.get_text(strip=True)\n",
        "            else:\n",
        "                review_text = \"No review text found\"\n",
        "\n",
        "            if stars:\n",
        "                number_of_stars = stars.get('data-service-review-rating')\n",
        "            else:\n",
        "                number_of_stars = \"No star rating found\"\n",
        "\n",
        "            csv_writer.writerow([name, review_text, number_of_stars])\n",
        "\n",
        "csv_filename = 'drivetime.csv'\n",
        "with open(csv_filename, 'w', encoding='utf-8', newline='') as csvfile:\n",
        "\n",
        "    csv_writer = csv.writer(csvfile)\n",
        "    csv_writer.writerow(['Name', 'Review', 'Number of Stars'])\n",
        "\n",
        "    for page_number in range(1, 51):\n",
        "        url = base_url.format(page_number)\n",
        "        scrape_reviews(url, csv_writer)\n",
        "\n",
        "print(f'Data has been written to {csv_filename}')"
      ],
      "metadata": {
        "id": "ntnfThpfI4yC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3066a22-c990-4817-a9b4-6b3a54406692"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data has been written to drivetime.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Website easyautoonline"
      ],
      "metadata": {
        "id": "VX4in1WUZXTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "name = \"easyautoonline\"\n",
        "base_url = 'https://www.trustpilot.com/review/easyautoonline.com?page={}'\n",
        "\n",
        "def scrape_reviews(url, csv_writer):\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        section_elements = soup.find_all('section', class_='styles_reviewContentwrapper__zH_9M')\n",
        "\n",
        "        for i, section_element in enumerate(section_elements, start=1):\n",
        "            review = section_element.find('p', class_='typography_body-l__KUYFJ typography_appearance-default__AAY17 typography_color-black__5LYEn')\n",
        "            stars = section_element.find('div', class_='styles_reviewHeader__iU9Px')\n",
        "\n",
        "            if review:\n",
        "                review_text = review.get_text(strip=True)\n",
        "            else:\n",
        "                review_text = \"No review text found\"\n",
        "\n",
        "            if stars:\n",
        "                number_of_stars = stars.get('data-service-review-rating')\n",
        "            else:\n",
        "                number_of_stars = \"No star rating found\"\n",
        "\n",
        "            csv_writer.writerow([name, review_text, number_of_stars])\n",
        "\n",
        "csv_filename = 'easyautoonline.csv'\n",
        "with open(csv_filename, 'w', encoding='utf-8', newline='') as csvfile:\n",
        "\n",
        "    csv_writer = csv.writer(csvfile)\n",
        "    csv_writer.writerow(['Name', 'Review', 'Number of Stars'])\n",
        "\n",
        "    for page_number in range(1, 51):\n",
        "        url = base_url.format(page_number)\n",
        "        scrape_reviews(url, csv_writer)\n",
        "\n",
        "print(f'Data has been written to {csv_filename}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Fz7ussVZMGn",
        "outputId": "cd7fa258-263c-4222-f6c9-f8687e3e9233"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data has been written to easyautoonline.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Clothes store"
      ],
      "metadata": {
        "id": "Kjvswbf-aYb4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Website selininy"
      ],
      "metadata": {
        "id": "yN1C9aBvZ0__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "name = \"selininy\"\n",
        "base_url = 'https://www.trustpilot.com/review/www.selininy.com?page{}'\n",
        "\n",
        "def scrape_reviews(url, csv_writer):\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        section_elements = soup.find_all('section', class_='styles_reviewContentwrapper__zH_9M')\n",
        "\n",
        "        for i, section_element in enumerate(section_elements, start=1):\n",
        "            review = section_element.find('p', class_='typography_body-l__KUYFJ typography_appearance-default__AAY17 typography_color-black__5LYEn')\n",
        "            stars = section_element.find('div', class_='styles_reviewHeader__iU9Px')\n",
        "\n",
        "            if review:\n",
        "                review_text = review.get_text(strip=True)\n",
        "            else:\n",
        "                review_text = \"No review text found\"\n",
        "\n",
        "            if stars:\n",
        "                number_of_stars = stars.get('data-service-review-rating')\n",
        "            else:\n",
        "                number_of_stars = \"No star rating found\"\n",
        "\n",
        "            csv_writer.writerow([name, review_text, number_of_stars])\n",
        "\n",
        "csv_filename = 'selininy.csv'\n",
        "with open(csv_filename, 'w', encoding='utf-8', newline='') as csvfile:\n",
        "\n",
        "    csv_writer = csv.writer(csvfile)\n",
        "    csv_writer.writerow(['Name', 'Review', 'Number of Stars'])\n",
        "\n",
        "    for page_number in range(1, 51):\n",
        "        url = base_url.format(page_number)\n",
        "        scrape_reviews(url, csv_writer)\n",
        "\n",
        "print(f'Data has been written to {csv_filename}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNmQVYEPZrYl",
        "outputId": "8048c752-065d-41d2-8cb1-e3976a1f86f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data has been written to selininy.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### website queensboro"
      ],
      "metadata": {
        "id": "bvtuhjvsaVSe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "name = \"queensboro\"\n",
        "base_url = 'https://www.trustpilot.com/review/www.queensboro.com?page={}'\n",
        "\n",
        "def scrape_reviews(url, csv_writer):\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        section_elements = soup.find_all('section', class_='styles_reviewContentwrapper__zH_9M')\n",
        "\n",
        "        for i, section_element in enumerate(section_elements, start=1):\n",
        "            review = section_element.find('p', class_='typography_body-l__KUYFJ typography_appearance-default__AAY17 typography_color-black__5LYEn')\n",
        "            stars = section_element.find('div', class_='styles_reviewHeader__iU9Px')\n",
        "\n",
        "            if review:\n",
        "                review_text = review.get_text(strip=True)\n",
        "            else:\n",
        "                review_text = \"No review text found\"\n",
        "\n",
        "            if stars:\n",
        "                number_of_stars = stars.get('data-service-review-rating')\n",
        "            else:\n",
        "                number_of_stars = \"No star rating found\"\n",
        "\n",
        "            csv_writer.writerow([name, review_text, number_of_stars])\n",
        "\n",
        "csv_filename = 'queensboro.csv'\n",
        "with open(csv_filename, 'w', encoding='utf-8', newline='') as csvfile:\n",
        "\n",
        "    csv_writer = csv.writer(csvfile)\n",
        "    csv_writer.writerow(['Name', 'Review', 'Number of Stars'])\n",
        "\n",
        "    for page_number in range(1, 51):\n",
        "        url = base_url.format(page_number)\n",
        "        scrape_reviews(url, csv_writer)\n",
        "\n",
        "print(f'Data has been written to {csv_filename}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAGXAwoPZ4ng",
        "outputId": "353f813d-b901-4514-8482-c54390ef90d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data has been written to queensboro.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###petrebellion"
      ],
      "metadata": {
        "id": "BLrbuTP-G6tl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "name = \"petrebellion\"\n",
        "base_url = 'https://www.trustpilot.com/review/petrebellion.co.uk?page={}'\n",
        "\n",
        "def scrape_reviews(url, csv_writer):\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        section_elements = soup.find_all('section', class_='styles_reviewContentwrapper__zH_9M')\n",
        "\n",
        "        for i, section_element in enumerate(section_elements, start=1):\n",
        "            review = section_element.find('p', class_='typography_body-l__KUYFJ typography_appearance-default__AAY17 typography_color-black__5LYEn')\n",
        "            stars = section_element.find('div', class_='styles_reviewHeader__iU9Px')\n",
        "\n",
        "            if review:\n",
        "                review_text = review.get_text(strip=True)\n",
        "            else:\n",
        "                review_text = \"No review text found\"\n",
        "\n",
        "            if stars:\n",
        "                number_of_stars = stars.get('data-service-review-rating')\n",
        "            else:\n",
        "                number_of_stars = \"No star rating found\"\n",
        "\n",
        "            csv_writer.writerow([name, review_text, number_of_stars])\n",
        "\n",
        "csv_filename = 'petrebellion.csv'\n",
        "with open(csv_filename, 'w', encoding='utf-8', newline='') as csvfile:\n",
        "\n",
        "    csv_writer = csv.writer(csvfile)\n",
        "    csv_writer.writerow(['Name', 'Review', 'Number of Stars'])\n",
        "\n",
        "    for page_number in range(1, 51):\n",
        "        url = base_url.format(page_number)\n",
        "        scrape_reviews(url, csv_writer)\n",
        "\n",
        "print(f'Data has been written to {csv_filename}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZdg9hRBGZ9o",
        "outputId": "e820ecec-d09b-41ce-a0da-ab7eeec20bd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data has been written to petrebellion.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###keystonepuppies"
      ],
      "metadata": {
        "id": "9_3xZbR2L-Q9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "name = \"keystonepuppies\"\n",
        "base_url = 'https://www.trustpilot.com/review/keystonepuppies.com?page={}'\n",
        "\n",
        "def scrape_reviews(url, csv_writer):\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        section_elements = soup.find_all('section', class_='styles_reviewContentwrapper__zH_9M')\n",
        "\n",
        "        for i, section_element in enumerate(section_elements, start=1):\n",
        "            review = section_element.find('p', class_='typography_body-l__KUYFJ typography_appearance-default__AAY17 typography_color-black__5LYEn')\n",
        "            stars = section_element.find('div', class_='styles_reviewHeader__iU9Px')\n",
        "\n",
        "            if review:\n",
        "                review_text = review.get_text(strip=True)\n",
        "            else:\n",
        "                review_text = \"No review text found\"\n",
        "\n",
        "            if stars:\n",
        "                number_of_stars = stars.get('data-service-review-rating')\n",
        "            else:\n",
        "                number_of_stars = \"No star rating found\"\n",
        "\n",
        "            csv_writer.writerow([name, review_text, number_of_stars])\n",
        "\n",
        "csv_filename = 'keystonepuppies.csv'\n",
        "with open(csv_filename, 'w', encoding='utf-8', newline='') as csvfile:\n",
        "\n",
        "    csv_writer = csv.writer(csvfile)\n",
        "    csv_writer.writerow(['Name', 'Review', 'Number of Stars'])\n",
        "\n",
        "    for page_number in range(1, 51):\n",
        "        url = base_url.format(page_number)\n",
        "        scrape_reviews(url, csv_writer)\n",
        "\n",
        "print(f'Data has been written to {csv_filename}')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEuv3Kx7La6q",
        "outputId": "f3d1b4e4-a7b1-4900-dd4a-256bddede04c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data has been written to keystonepuppies.csv\n"
          ]
        }
      ]
    }
  ]
}